\section{Multistep Q-Learning}

\ifsolutions
\input{hw3_solutions/01_solutions}
\fi

\def\solve#1{\csname solution to #1\endcsname}

\begingroup
\def\Q{Q_{\phi_k}}
\def\Qn{Q_{\phi_{k+1}}}
\def\D{\mathcal{D}}

Consider the $N$-step variant of Q-learning described in lecture. We learn $\Qn$ with the following updates:\begin{align}
  y_{j,t} &\gets \biggl(\;\sum_{t'=t}^{t+N-1} \gamma^{t'-t} r_{j,t'}\biggr)+\gamma^{N} \max _{\mathbf{a}_{j,t+N}} \Q\left(\mathbf{s}_{j,t+N}, \mathbf{a}_{j,t+N}\right) \label{eq:q_target}\\
  \phi_{k+1} &\gets \underset{\phi\in\Phi}{\arg\min}  \sum_{j,t} \bigl( y_{j,t}-Q_{\phi}(\mathbf s_{j,t},\mathbf a_{j,t}) \bigr)^2 \label{eq:q_update}
\end{align}
In these equations, $j$ indicates an index in the replay buffer of trajectories $\D_k$. We first roll out a batch of $B$ trajectories to update $\D_k$ and compute the target values in \eqref{eq:q_target}. We then fit $\Qn$ to these target values with \eqref{eq:q_update}. 
After estimating $\Qn$, we can then update the policy through an argmax:\begin{align}
  \pi_{k+1}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\gets \left\{\begin{array}{l}1 \text { if } \mathbf{a}_{t}=\arg \max _{\mathbf{a}_{t}} Q_{\phi_{k+1}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \\ 0 \text{ otherwise. }\end{array}\right. \label{eq:policy_improvement}
\end{align}
We repeat the steps in \cref{eq:q_target,eq:q_update,eq:policy_improvement} $K$ times to improve the policy. In this question, you will analyze some properties of this algorithm, which is summarized in \Cref{alg:multi}.

\begin{algorithm}
\caption{Multistep Q-Learning}
\label{alg:multi}
\begin{algorithmic}[1]
	\Require{iterations $K$, batch size $B$}
	\State initialize random policy $\pi_0$, sample $\phi_0\sim\Phi$
	\For{$k=0\ldots K-1$}
		\State Update $\D_{k+1}$ with $B$ new rollouts from $\pi_k$ \label{eq:data}
		\State compute targets with \eqref{eq:q_target}
		\State $Q_{\phi_{k+1}} \gets$ update with \eqref{eq:q_update}
		\State $\pi_{k+1} \gets$ update with \eqref{eq:policy_improvement}
	\EndFor
	\State\Return $\pi_{K}$
\end{algorithmic}	
\end{algorithm}



\def\makecols#1#2{{\def\p{#2}\newcount\i\i0\hfill\loop\advance\i1\makebox[1cm][c]{\expandafter\p\the\i}\kern.5cm\ifnum\i<#1\repeat\kern-1cm}}
\def\heading#1{\bf\expandafter\uppercase\expandafter{\romannumeral#1.}}
\def\boxes#1{\ensuremath\square}
\def\filled#1#2|#3{\ifnum#1=#3\ensuremath\blacksquare
	\else\if\relax#2\relax\ensuremath\square
	\else\filled#2|#3\fi\fi}
\def\ncol{3}
\newcommand{\checkeditem}[2]{\edef\x{0#1}\item[\expandafter\filled\x|#2]}


\def\choices#1#2{
	\begin{enumerate}
	\item on-policy in tabular setting \makecols\ncol{\filled0#1|}
	\item off-policy in tabular setting \makecols\ncol{\filled0#2|}
	\end{enumerate}
}

\subsection{TD-Learning Bias (2 points)}
\label{q:td_bias}

\def\answer{} % <--- TODO: insert index (0/1) of answer
\ifsolutions\solve\thesubsection\fi
We say an estimator $f_\D$ of $f$ constructed using data $\D$ sampled from process $P$ is \textit{unbiased} when $\mathbb{E}_{\D\sim P}[f_\D(x)-f(x)]=0$ at each $x$.

Assume $\hat Q$ is a noisy (but unbiased) estimate for $Q$. Is the Bellman backup $\mathcal{B}\hat Q = r(s, a) + \gamma \max_{a'} \hat Q(s', a')$ an unbiased estimate of $\mathcal{B}Q$?
\begin{itemize}
    \checkeditem\answer1 Yes
    \checkeditem\answer2 No
\end{itemize}

\textbf{Solution:}

æˆ‘ä»¬çŸ¥é“ï¼š

* $\hat Q$ æ˜¯æ— åçš„ï¼š

  $$
  \mathbb{E}[\hat Q(s,a)] = Q(s,a)
  $$

ä½†æ˜¯ç°åœ¨æˆ‘ä»¬ä¸æ˜¯ç›´æ¥ç”¨ $\hat Q$ï¼Œè€Œæ˜¯ç”¨äº†ä¸€ä¸ªéçº¿æ€§æ“ä½œï¼š

$$
\max_{a'} \hat Q(s', a')
$$

é—®é¢˜å°±å‡ºåœ¨è¿™é‡Œã€‚

---

âŒ éçº¿æ€§æ“ä½œç ´åæ— åæ€§

â€œmaxâ€ æ˜¯ä¸€ä¸ª**éçº¿æ€§ç®—å­**ï¼Œå› æ­¤ï¼š

$$
\mathbb{E}[\max_{a'} \hat Q(s', a')] \neq \max_{a'} \mathbb{E}[\hat Q(s', a')]
$$

æ¢å¥è¯è¯´ï¼Œå³ä½¿æ¯ä¸ªåŠ¨ä½œçš„ $\hat Q(s',a')$ éƒ½æ˜¯æ— åçš„ï¼Œ
å–æœ€å¤§å€¼ä¹‹åï¼ŒæœŸæœ›å€¼ä¼š **åé«˜**ï¼ˆoverestimateï¼‰ã€‚

---

ğŸ” ä¸¾ä¸ªç®€å•ä¾‹å­

å‡è®¾åªæœ‰ä¸¤ä¸ªåŠ¨ä½œ $a_1, a_2$ï¼Œå®ƒä»¬çš„çœŸå®å€¼ä¸€æ ·ï¼š

$$
Q(s',a_1) = Q(s',a_2) = 1
$$

è€Œä¼°è®¡æœ‰å™ªå£°ï¼š

$$
\hat Q(s',a_1) = 1 + \epsilon_1,\quad \hat Q(s',a_2) = 1 + \epsilon_2
$$

å…¶ä¸­ $\epsilon_i$ æ˜¯é›¶å‡å€¼å™ªå£°ï¼ˆå³æ— åï¼‰ã€‚

é‚£æˆ‘ä»¬æœ‰ï¼š

$$
\mathbb{E}[\max(\hat Q_1, \hat Q_2)] = 1 + \mathbb{E}[\max(\epsilon_1, \epsilon_2)] > 1
$$

æ‰€ä»¥å³ä½¿å•ä¸ªä¼°è®¡æ— åï¼Œå–æœ€å¤§å€¼åæ•´ä½“**åé«˜**ã€‚

---

ğŸ§© ç»“è®º

å› æ­¤ï¼š

$$
\boxed{
\mathcal{B}\hat Q = r + \gamma \max_{a'} \hat Q(s',a')
}
$$

å¹¶ä¸æ˜¯ $\mathcal{B}Q = r + \gamma \max_{a'} Q(s',a')$ çš„æ— åä¼°è®¡ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œ**Bellman backup æœ‰æ­£åå·®ï¼ˆoverestimation biasï¼‰**ã€‚

æ­£ç¡®ç­”æ¡ˆæ˜¯ï¼š**No**ã€‚

\subsection{Tabular Learning (6 points total)}
\label{q:tabular_learning}

At each iteration of the algorithm above after the update from \cref{eq:q_update}, $\Q$ can be viewed as an estimate of the true optimal $Q^*$. Consider the following statements: 
\begin{enumerate}[label=\bf\Roman*.]
  \item $Q_{\phi_{k+1}}$ is an unbiased estimate of the $Q$ function of the last policy, $Q^{\pi_k}$.
  \item As $k\to\infty$ for some fixed $B$, $\Q$ is an unbiased estimate of $Q^*$, i.e., $\lim_{k\to\infty} \mathbb{E}\bigl[\Q(s,a)-Q^*(s,a)]=0$.
  \item In the limit of infinite iterations and data we recover the optimal $Q^*$, i.e., $\lim_{k,B\to\infty}\mathbb{E}\,\bigl[\|\Q-Q^*\|_\infty\bigr]=0$.
\end{enumerate}

We make the additional assumptions: 
\begin{itemize}
	\item The state and action spaces are finite.
	\item Every batch contains at least one experience for each action taken in each state.
	\item In the tabular setting, $\Q$ can express any function, i.e., $\{\Q:\phi\in\Phi\}=\mathbb{R}^{S\times A}$.
\end{itemize}
When updating the buffer $\D_k$ with $B$ new trajectories in \cref{eq:data} of \Cref{alg:multi}, we say:
\begin{itemize}
    \item When learning \textit{on-policy}, $\D_k$ is set to contain only the set of $B$ new rollouts of $\pi$ (so $\lvert \D_k \rvert = B$). Thus, we only train on rollouts from the current policy.
	\item When learning \textit{off-policy}, we use a fixed dataset $\D_k=\D$ of $B$ trajectories from another policy $\pi'$. 
\end{itemize}

Indicate which of the statements \textbf{I-III} always hold in the following cases. No justification is required.
\ifsolutions\solve\thesubsection\else
\begin{enumerate}
\item $N=1$ and \ldots \makecols\ncol\heading\choices
	{} % <--- TODO: select numbers of boxes to fill; e.g., {13} to select I and III
	{} % <--- TODO: "
\item $N>1$ and \ldots \choices
	{} % <--- TODO: "
	{} % <--- TODO: "
\item In the limit as $N\to\infty$ (no bootstrapping) \ldots \choices
	{} % <--- TODO: "
	{} % <--- TODO: "
\end{enumerate}
\fi

\textbf{Solution:}
å¥½ï½æˆ‘ä»¬æŠŠé¢˜ç›®é‡Œçš„ä¸‰æ¡æ–­è¨€ **Iâ€“III**ï¼Œåœ¨ 6 ä¸ªåœºæ™¯ï¼ˆ$N{=}1$/$N{>}1$/$N{\to}\infty$ Ã— on/off-policyï¼‰**é€ä¸€**è§£é‡Šæ¸…æ¥šã€‚å…ˆå›é¡¾ä¸‰æ¡æ–­è¨€å«ä¹‰ï¼š

* **I**ï¼šä¸€æ¬¡æ›´æ–°åï¼Œ$Q_{\phi_{k+1}}$ æ˜¯**ä¸Šä¸€è½®ç­–ç•¥** $\pi_k$ çš„ $Q^{\pi_k}$ çš„**æ— åä¼°è®¡**ã€‚

* **II**ï¼šå›ºå®šæ‰¹é‡å¤§å° $B$ï¼Œå½“ $k\to\infty$ æ—¶ï¼Œ$Q_{\phi_k}$ æ˜¯ $Q^*$ çš„**æ— åä¼°è®¡**ï¼ˆ$\lim_{k\to\infty}\mathbb E[Q_{\phi_k}-Q^*]=0$ï¼‰ã€‚

* **III**ï¼šå½“ $k,B\to\infty$ï¼ˆæ— é™è¿­ä»£ä¸”æ¯æ¬¡æœ‰æ— é™æ•°æ®ï¼‰æ—¶ï¼Œæ¢å¤æœ€ä¼˜ $Q^*$ï¼ˆ$\lim_{k,B\to\infty}\mathbb E\|Q_{\phi_k}-Q^*\|_\infty=0$ï¼‰ã€‚

é¢˜ç›®å‡è®¾ï¼š**tabular** è¡¨ç¤ºèƒ½åŠ›å®Œå¤‡ï¼›çŠ¶æ€åŠ¨ä½œæœ‰é™ï¼›æ¯ä¸ªè®­ç»ƒ batch å¯¹æ¯ä¸ª $(s,a)$ éƒ½æœ‰è‡³å°‘ä¸€æ¬¡æ ·æœ¬ï¼ˆè¦†ç›–æ€§ï¼‰ã€‚

---

é¢„å¤‡ï¼šå‡ ä¸ªç®—å­ä¸ N æ­¥ç›®æ ‡

* **ç­–ç•¥è¯„ä¼°ç®—å­**ï¼š$(\mathcal T^{\pi}Q)(s,a)=r(s,a)+\gamma\,\mathbb E_{s'}\mathbb E_{a'\sim\pi}[Q(s',a')]$

* **æœ€ä¼˜ç®—å­**ï¼š$(\mathcal T^*Q)(s,a)=r(s,a)+\gamma\,\mathbb E_{s'}[\max_{a'}Q(s',a')]$ï¼ˆæ”¶æ•›ä¸åŠ¨ç‚¹ä¸º $Q^*$ï¼‰

* **N æ­¥æœ€ä¼˜ backup**ï¼ˆé¢˜é‡Œç›®æ ‡ï¼‰ï¼š

  $$
  y_{t}=\sum_{i=0}^{N-1}\gamma^i r_{t+i}+\gamma^N\max_{a}Q(s_{t+N},a).
  $$

  on-policy æ—¶ï¼Œå‰ $N{-}1$ æ­¥åŠ¨ä½œæ¥è‡ª $\pi_k$ï¼›off-policy æ—¶ï¼Œæ¥è‡ªè¡Œä¸ºç­–ç•¥ $\pi'$ã€‚

è¦ç‚¹ï¼šåªè¦ç›®æ ‡é‡Œå‡ºç° **$\max$**ï¼Œå®ƒå¯¹åº”çš„æ˜¯æœ **æœ€ä¼˜ç®—å­ $\mathcal T^*$** çš„æ›´æ–°ï¼Œè€Œä¸æ˜¯å¯¹ $\pi_k$ çš„è¯„ä¼° $\mathcal T^{\pi_k}$ã€‚

åœ¨å¼ºåŒ–å­¦ä¹ é‡Œï¼Œä¸€ä¸ªç­–ç•¥ Ï€ å†³å®šäº†åŠ¨ä½œé€‰æ‹©æ–¹å¼ï¼š

$$
a_t \sim \pi(\cdot|s_t)
$$

â— on-policy

* ä½ åœ¨å­¦ä¹  $Q^{\pi}$ï¼ˆå½“å‰ç­–ç•¥çš„ä»·å€¼å‡½æ•°ï¼‰ã€‚
* æ•°æ®æ ·æœ¬ä¹Ÿæ˜¯ç”±åŒä¸€ä¸ªç­–ç•¥ Ï€ äº§ç”Ÿçš„ã€‚

å³ï¼š

$$
\text{é‡‡æ ·ç­–ç•¥} = \text{å­¦ä¹ ç›®æ ‡ç­–ç•¥} = \pi
$$

ä¸¾ä¾‹ï¼š
ä½ ç”¨å½“å‰çš„ Îµ-greedy ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶ç”¨è¿™äº›æ•°æ®æ¥æ›´æ–°è‡ªå·±çš„ Q å€¼ã€‚
â†’ è¿™æ˜¯ on-policyï¼ˆæ¯”å¦‚ **SARSA** ç®—æ³•ï¼‰ã€‚

---

â— off-policy

* ä½ æƒ³å­¦ä¹  $Q^{\pi}$ï¼ˆç›®æ ‡ç­–ç•¥çš„ä»·å€¼å‡½æ•°ï¼‰ï¼Œ
  ä½†æ•°æ®æ˜¯ç”±å¦ä¸€ä¸ªç­–ç•¥ Ï€â€² äº§ç”Ÿçš„ã€‚

å³ï¼š

$$
\text{é‡‡æ ·ç­–ç•¥ } \pi' \;\neq\; \text{ç›®æ ‡ç­–ç•¥ } \pi
$$

ä¸¾ä¾‹ï¼š
ä½ åœ¨çœ‹åˆ«äººç©æ¸¸æˆï¼ˆÏ€â€²ï¼‰ï¼Œä½†ä½ æƒ³å­¦ä¹ æœ€ä¼˜ç­–ç•¥ Ï€\*ã€‚â†’ è¿™æ˜¯ off-policyï¼ˆæ¯”å¦‚ **Q-learning** ç®—æ³•ï¼‰ã€‚

On-policy æ›´æ–°ï¼ˆä¾‹å¦‚ SARSAï¼‰ï¼š

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\big]
$$

ğŸ‘‰ ä¸‹ä¸€æ­¥åŠ¨ä½œ $a_{t+1}$ æ˜¯æŒ‰ç…§**å½“å‰ç­–ç•¥ Ï€** é€‰çš„ã€‚

---

Off-policy æ›´æ–°ï¼ˆä¾‹å¦‚ Q-learningï¼‰ï¼š

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\big]
$$

ğŸ‘‰ ä¸‹ä¸€æ­¥ä½¿ç”¨ $\max_{a'}$ï¼Œè¡¨ç¤ºå­¦ä¹ çš„æ˜¯**æœ€ä¼˜ç­–ç•¥ Ï€\***ï¼Œ
è€Œä¸æ˜¯ç”Ÿæˆæ•°æ®çš„è¡Œä¸ºç­–ç•¥ã€‚

---

1) $N=1$

(a) on-policyï¼ˆæ¯æ¬¡ç”¨ $\pi_k$ é‡‡æ · B æ¡æ–°è½¨è¿¹ï¼Œä»…ç”¨æœ¬æ¬¡æ•°æ®è®­ç»ƒï¼‰

* **Iï¼šä¸æˆç«‹ã€‚** ä¸€æ­¥ç›®æ ‡æ˜¯ $r+\gamma\max_{a'}Q_k(s',a')$ï¼Œè¿™æ˜¯ $\mathcal T^*Q_k$ çš„æ ·æœ¬è¿‘ä¼¼ï¼Œä¼°è®¡çš„æ˜¯æœ€ä¼˜ç­–ç•¥è€Œä¸æ˜¯ä¸Šä¸€è½®çš„ç­–ç•¥ï¼Œä¸æ˜¯ $\mathcal T^{\pi_k}Q_k$ã€‚æ‰€ä»¥å¹¶éåœ¨æ— åè¯„ä¼° $Q^{\pi_k}$ã€‚

* **IIï¼šä¸æˆç«‹ã€‚** $B$ å›ºå®šã€æ¯æ¬¡åªç”¨å½“è½®æœ‰é™æ ·æœ¬ï¼Œä¸”ç›®æ ‡å« $\max$ï¼ˆ**maximization bias**ï¼‰ã€‚å³ä¾¿ $k\to\infty$ï¼Œâ€œæ— åä¼°è®¡ $Q^*$â€ å¤ªå¼ºï¼Œ$\mathbb{E}[\max_{a'} \hat Q(s', a')] \neq \max_{a'} \mathbb{E}[\hat Q(s', a')]$è¾¾ä¸åˆ°ã€‚

* **IIIï¼šæˆç«‹ã€‚** åœ¨ tabular + å®Œå…¨è¦†ç›– + $B\to\infty$ çš„æé™ä¸‹ï¼Œæ¯æ¬¡ backup é€¼è¿‘ $\mathcal T^*$ï¼Œè€Œ $\mathcal T^*$ åœ¨ $\|\cdot\|_\infty$ ä¸‹æ˜¯ $\gamma$-æ”¶ç¼©æ˜ å°„ï¼Œè¿­ä»£æ”¶æ•›åˆ°å”¯ä¸€ä¸åŠ¨ç‚¹ $Q^*$ã€‚

(b) off-policyï¼ˆå›ºå®šä¸€ä¸ªæ¥è‡ª $\pi'$ çš„æ•°æ®é›† $\mathcal D$ï¼‰

* **Iï¼šä¸æˆç«‹ã€‚** åŒä¸Šï¼Œç›®æ ‡æ˜¯æœ $\mathcal T^*$ è€Œé $\mathcal T^{\pi_k}$ï¼›ä¸”æ•°æ®åˆ†å¸ƒä¸ $\pi_k$ ä¸åŒï¼Œæ›´ä¸æ»¡è¶³â€œæ— åè¯„ä¼° $Q^{\pi_k}$â€ã€‚

* **IIï¼šä¸æˆç«‹ã€‚** ç†ç”±åŒä¸Šã€‚

* **IIIï¼šä¸æˆç«‹ã€‚** å¦‚æœæ•°æ®å¯¹æ¯ä¸ª $(s,a)$ æœ‰è¦†ç›–ï¼Œ**Fitted Q-Iterationï¼ˆFQIï¼‰**ï¼š$B\to\infty$ æ—¶ç»éªŒ backup $\to \mathcal T^*$ï¼Œé‡å¤è¿­ä»£æ”¶æ•›åˆ° $Q^*$ï¼ˆä¸è¡Œä¸ºç­–ç•¥æ— å…³ï¼Œå› ä¸ºä¸€æ­¥ backup ä¸­å¹¶ä¸éœ€è¦ä¿®æ­£åˆ†å¸ƒï¼‰ã€‚$y_t = r_t + \gamma \max_{a'} Q(s_{t+1}, a')$è¿™ä¸ªä¸åŒ…å«ä»»ä½•æ¥è‡ªè¡Œä¸ºç­–ç•¥ Ï€â€² çš„åŠ¨ä½œé€‰æ‹©ï¼Œå³æ—¶å¥–åŠ±$r_{t}$åªç”±å½“å‰åŠ¨ä½œçŠ¶æ€å†³å®šå’Œä¸‹ä¸€ä¸ªçŠ¶æ€$s_{t+1}$æ˜¯ç”±åˆ†å¸ƒç¯å¢ƒ$P(s_{t+1}|s_t,a_t)$å†³å®šçš„ã€‚ä½†æ˜¯æ­¤å¤„çš„ï¼šLearning off-policy with fixed dataset $\mathcal D$ of $B$ trajectories from another policy $\pi'$ï¼Œå¦‚æœæ•°æ®é›†$\mathcal D$ä¸­æ²¡æœ‰è¦†ç›–æ‰€æœ‰çš„$(s,a)$å¯¹ï¼Œé‚£ä¹ˆå°±æ— æ³•ä¿è¯æ”¶æ•›åˆ°$Q^*$ã€‚å› æ­¤IIIä¸æˆç«‹ã€‚

2) $N>1$ï¼ˆæœ‰é™å¤šæ­¥ï¼‰

(a) on-policy

* **Iï¼šä¸æˆç«‹ã€‚** ç›®æ ‡ï¼šå‰ $N$ æ­¥æ˜¯çœŸå®å›æŠ¥ï¼Œä½†**ç¬¬ $N$ æ­¥ç”¨ $\max$ è‡ªä¸¾**ï¼Œå®ƒå¹¶é $\mathcal T^{\pi_k}$ï¼ˆåè€…åº”åœ¨æ¯ä¸€æ­¥éƒ½å¯¹ $\pi_k$ å–æœŸæœ›ï¼‰ã€‚

* **IIï¼šä¸æˆç«‹ã€‚** ä¾æ—§æ˜¯æœ‰é™æ ·æœ¬ + $\max$ å¸¦æ¥çš„åå·®ï¼Œå›ºå®š $B$ ä¸èƒ½ä¿è¯â€œæ— ååˆ° $Q^*$â€ã€‚

* **IIIï¼šæˆç«‹ã€‚** åœ¨ tabularã€è¦†ç›–ä¸” $B\to\infty$ çš„æé™ä¸‹ï¼ŒN æ­¥æœ€ä¼˜ backup çš„æœŸæœ›ç­‰ä»·äºæŠŠ $\mathcal T^*$ ç½®äºç¬¬ $N$ æ­¥ï¼ˆå¯è§†ä½œ $\gamma^N$-æ”¶ç¼©ï¼‰ï¼Œé…åˆè´ªå¿ƒæ”¹è¿›å½¢æˆå¹¿ä¹‰ç­–ç•¥è¿­ä»£ï¼Œæœ€ç»ˆæ”¶æ•›åˆ° $Q^*$ã€‚

(b) off-policy

* **Iï¼šä¸æˆç«‹ã€‚** å‰ $N{-}1$ æ­¥çš„å¥–åŠ±æ¥è‡ªè¡Œä¸ºç­–ç•¥ $\pi'$ çš„è½¨è¿¹ï¼Œç„¶è€Œç›®æ ‡è¦â€œæœæœ€ä¼˜â€ï¼Œåˆ†å¸ƒä¸åŒ¹é…ä¸”æ— ä¿®æ­£ï¼Œ**ä¸æ˜¯**å¯¹ $Q^{\pi_k}$ çš„æ— åè¯„ä¼°ã€‚

* **IIï¼šä¸æˆç«‹ã€‚** åŒä¸Šã€‚

* **IIIï¼šä¸æˆç«‹ã€‚** å¤šæ­¥ off-policy **å¦‚æœä¸åšé‡è¦æ€§é‡‡æ ·æ ¡æ­£**ï¼Œå³ä½¿æ•°æ®æ— é™ï¼Œä¹Ÿä¼šæ”¶æ•›åˆ°é”™è¯¯çš„å›ºå®šç‚¹ï¼ˆN æ­¥å›æŠ¥æ··å…¥äº† $\pi'$ çš„å†³ç­–ï¼‰ï¼Œä¸èƒ½ä¿è¯å¾—åˆ° $Q^*$ã€‚

å½“ $N>1$ æ—¶ï¼Œ
\[
y_t^{(N)} = r_t + \gamma r_{t+1} + \cdots + \gamma^{N-1} r_{t+N-1}
+ \gamma^N \max_a Q(s_{t+N}, a)
\]

\noindent é—®é¢˜ï¼šè¿™äº› $r_{t+i}$ æ¥è‡ªè°ï¼Ÿ

\begin{quote}
ğŸ‘‰ æ¥è‡ªè¡Œä¸ºç­–ç•¥ $\pi'$ åœ¨ç¯å¢ƒä¸­â€œè¿ç»­é€‰æ‹©çš„åŠ¨ä½œâ€æ‰€äº§ç”Ÿçš„è½¨è¿¹ã€‚
\end{quote}

äºæ˜¯ï¼š

\begin{itemize}
  \item å¥–åŠ±åºåˆ—çš„ç»Ÿè®¡ç‰¹æ€§å–å†³äº $\pi'$ï¼›
  \item ä¹Ÿå°±æ˜¯è¯´ï¼Œ
  \[
  \mathbb{E}_{\pi'}[\,r_t + \gamma r_{t+1} + \dots + \gamma^{N-1} r_{t+N-1}\,]
  \;\neq\;
  \mathbb{E}_{\pi^*}[\cdots].
  \]
\end{itemize}

\noindent æ¢å¥è¯è¯´ï¼š

\begin{quote}
ä½ ç”¨ $\pi'$ çš„è¡Œä¸ºè½¨è¿¹å»ä¼°è®¡ $\pi^*$ çš„æœªæ¥å›æŠ¥ï¼ŒæœŸæœ›é”™äº†ã€‚
\end{quote}

è¿™å¯¼è‡´å¤šæ­¥ç›®æ ‡ä¸å†æ˜¯ $\mathcal{T}^*$ çš„æœŸæœ›ï¼Œè€Œæ˜¯æŸä¸ªæ··åˆç®—å­ï¼š

\[
\tilde{\mathcal{T}}_{N}^{\pi'}(Q)
= \mathbb{E}_{\pi'}\!\left[
  \sum_{i=0}^{N-1}\gamma^i r_{t+i}
  + \gamma^N \max_a Q(s_{t+N}, a)
\right]
\]

\noindent è¿™ä¸ªç®—å­çš„å›ºå®šç‚¹ä¸å†æ˜¯ $Q^*$ã€‚

---

3) $N\to\infty$ï¼ˆçº¯ Monte Carloï¼Œæ— è‡ªä¸¾ï¼‰

(a) on-policy

* **Iï¼šæˆç«‹ã€‚** ç›®æ ‡å°±æ˜¯å®Œæ•´å›æŠ¥ $G_t$ï¼›on-policy ä¸‹ $\mathbb E[G_t|s,a]=Q^{\pi_k}(s,a)$ï¼ŒMC è¯„ä¼°å¯¹ $Q^{\pi_k}$ **æ— å**ã€‚

* **IIï¼šä¸æˆç«‹ã€‚** é¢˜ç›®è¦æ±‚â€œå›ºå®š $B$â€ï¼Œæ¯è½®åªç”¨ $B$ æ¡æ–°æ ·æœ¬åšè¯„ä¼°å¹¶ç«‹åˆ»è´ªå¿ƒæ”¹è¿›ï¼Œå¸¦å™ªå£°çš„æ”¹è¿›è¿‡ç¨‹ä¸ä¿è¯â€œ$\lim_{k\to\infty}$ æ— ååˆ° $Q^*$â€ï¼›è¿™ä¸ªæ–­è¨€æ¯”â€œæ”¶æ•›â€æ›´å¼ºï¼Œä¸€èˆ¬ä¸æˆç«‹ã€‚

* **IIIï¼šæˆç«‹ã€‚** $B\to\infty$ æ—¶ï¼ŒMC å¯¹ $Q^{\pi_k}$ çš„ä¼°è®¡ä¸€è‡´ï¼›ä¸è´ªå¿ƒæ”¹è¿›äº¤æ›¿ï¼ˆMC Policy Iterationï¼‰åœ¨ tabular ä¸‹æ”¶æ•›åˆ° $Q^*$ã€‚

(b) off-policy

* **Iï¼šä¸æˆç«‹ã€‚** MC **off-policy** è‹¥ä¸åšé‡è¦æ€§é‡‡æ ·ï¼Œ$\mathbb E_{\pi'}[G_t]\neq Q^{\pi_k}$ï¼Œè¯„ä¼°æœ‰ç³»ç»Ÿåå·®ã€‚æƒ³å¾—åˆ°$Q^{\pi_k}$ï¼Œå¿…é¡»ç”¨ä½†æ˜¯é‡‡æ ·æ˜¯åœ¨$\pi'$ä¸‹è¿›è¡Œçš„è½¨è¿¹ï¼Œéœ€è¦ç”¨é‡è¦æ€§é‡‡æ ·æ ¡æ­£ã€‚

* **IIï¼šä¸æˆç«‹ã€‚** åŒç†ä¸Šï¼Œæ²¡æœ‰é‡è¦æ€§é‡‡æ ·æ ¡æ­£ï¼ŒMC off-policy è¯„ä¼°æœ‰åå·®ï¼Œä¸èƒ½ä¿è¯æ— ååˆ° $Q^*$ã€‚

* **IIIï¼šä¸æˆç«‹ã€‚** æ—  IS æ ¡æ­£æ—¶ï¼ŒMC off-policy ä¸èƒ½ä¿è¯ä¸€è‡´æ€§ï¼Œæ›´è°ˆä¸ä¸Šæ”¶æ•›åˆ° $Q^*$ã€‚å³ä¾¿ $B \to \infty$ã€$k \to \infty$ï¼Œè‹¥ä¸åš IS/æ ¡æ­£ï¼ŒMC off-policy çš„æœŸæœ›æŒ‡å‘çš„ä¸æ˜¯$\mathbb{E}_{\pi}[\cdot]$ï¼Œè€Œæ˜¯ $\mathbb{E}_{\pi'}[\cdot]$ã€‚äºæ˜¯å®ƒæ”¶æ•›åˆ°é”™è¯¯çš„å›ºå®šç‚¹ï¼ˆåæ˜  $\pi'$ è½¨è¿¹çš„é•¿æœŸå›æŠ¥ï¼‰ï¼Œä¸æ˜¯ $Q^*$ï¼Œå› æ­¤ III ä¸æˆç«‹ã€‚

\subsection{Variance of $Q$ Estimate (2 points)}
\label{q:variance_estimate}
Which of the three cases ($N = 1$, $N > 1$, $N \to \infty$) would you expect to have the highest-variance estimate of $Q$ for fixed dataset size $B$ in the limit of infinite iterations $k$? Lowest-variance?

\def\highest{} % <--- TODO: insert index of highest variance answer
\def\lowest{} % <--- TODO: insert index of lowest variance answer
\ifsolutions\solve\thesubsection\fi
\begin{minipage}{0.49\linewidth}
Highest variance:\smallskip
\begin{itemize}\itemsep=1ex
    \checkeditem\highest1 $N = 1$
    \checkeditem\highest2 $N > 1$
    \checkeditem\highest3 $N \to \infty$
\end{itemize}
\end{minipage}
\begin{minipage}{0.49\linewidth}
Lowest variance:\smallskip
\begin{itemize}\itemsep=1ex
    \checkeditem\lowest1 $N = 1$
    \checkeditem\lowest2 $N > 1$
    \checkeditem\lowest3 $N \to \infty$
\end{itemize}
\end{minipage}


\textbf{Solution}
**ç­”æ¡ˆï¼š**

* **Highest varianceï¼ˆæ–¹å·®æœ€é«˜ï¼‰ï¼š** $N \to \infty$

* **Lowest varianceï¼ˆæ–¹å·®æœ€ä½ï¼‰ï¼š** $N = 1$

**ç†ç”±ï¼ˆç®€è¿°ï¼‰ï¼š**
$N=1$ï¼ˆä¸€æ­¥ TD/Q-learningï¼‰åœ¨ç›®æ ‡ä¸­ç«‹å³**è‡ªä¸¾**ï¼Œåªç”¨å½“å‰å¥–åŠ± $r_t$ åŠ ä¸Š $\gamma \max_a Q(s_{t+1},a)$ çš„ä¼°è®¡ï¼Œå› è€Œç›®æ ‡çš„éšæœºæˆåˆ†æœ€å°‘ï¼Œ**æ–¹å·®æœ€ä½**ï¼ˆä½†åå·®è¾ƒå¤§ï¼‰ã€‚
$N\to\infty$ï¼ˆçº¯ MCï¼‰æŠŠæ•´æ¡æœªæ¥å›æŠ¥ $\sum_{i=0}^{T-t-1}\gamma^i r_{t+i}$ éƒ½å½“ä½œç›®æ ‡ï¼Œç´¯ç§¯äº†æ‰€æœ‰éšæœºæ€§ï¼ˆè½¬ç§»ã€å¥–åŠ±ã€è½¨è¿¹é•¿åº¦ç­‰ï¼‰ï¼Œ**æ–¹å·®æœ€é«˜**ï¼ˆä½†æ— è‡ªä¸¾åå·®ï¼‰ã€‚
$N>1$ è½åœ¨ä¸¤è€…ä¹‹é—´ï¼Œæ–¹å·®ä»‹äºäºŒè€…ä¹‹é—´ã€‚å›ºå®šæ‰¹é‡ $B$ æ—¶ï¼Œå³ä½¿ $k\to\infty$ï¼Œæ¯æ¬¡ç›®æ ‡çš„é‡‡æ ·å™ªå£°ä¸ä¼šæ¶ˆå¤±ï¼Œå› æ­¤è¿™ç§æ–¹å·®æ’åºæˆç«‹

\subsection{Function Approximation (2 points)}
\label{q:function_approximation}
Now say we want to represent $Q$ via function approximation rather than with a tabular representation. Assume that for any deterministic policy $\pi$ (including the optimal policy $\pi^*$), function approximation can represent the true $Q^\pi$ exactly.
Which of the following statements are true?

\def\answer{} % <--- TODO: insert index of answer(s)
\ifsolutions\solve\thesubsection\fi
\begin{itemize}
    \checkeditem\answer1 When $N = 1$, $Q_{\phi_{k+1}}$ is an unbiased estimate of the $Q$-function of the last policy $Q^{\pi_k}$.
    \checkeditem\answer2 When $N = 1$ and in the limit as $B\to\infty,\,k \to \infty$, $\Q$ converges to $Q^*$.
    \checkeditem\answer3 When $N > 1$ (but finite) and in the limit as $B\to\infty,\,k \to \infty$, $\Q$ converges to $Q^*$.
    \checkeditem\answer4 When $N \to \infty$ and in the limit as $B \to \infty,\,k \to \infty$, $\Q$ converges to $Q^*$.
\end{itemize}

\textbf{Solution:}

ç°åœ¨æˆ‘ä»¬å¸Œæœ›é€šè¿‡**å‡½æ•°é€¼è¿‘ï¼ˆfunction approximationï¼‰**æ¥è¡¨ç¤º $Q$ï¼Œ
è€Œä¸æ˜¯ä½¿ç”¨**è¡¨æ ¼å‹è¡¨ç¤ºï¼ˆtabular representationï¼‰**ã€‚

å‡è®¾ï¼š
å¯¹äºä»»æ„ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ $\pi$ï¼ˆåŒ…æ‹¬æœ€ä¼˜ç­–ç•¥ $\pi^*$ï¼‰ï¼Œ
å‡½æ•°é€¼è¿‘å™¨éƒ½å¯ä»¥**ç²¾ç¡®åœ°è¡¨ç¤ºè¯¥ç­–ç•¥çš„ Q å‡½æ•° $Q^\pi$**ã€‚

é—®ï¼šä»¥ä¸‹å“ªäº›é™ˆè¿°æ˜¯æ­£ç¡®çš„ï¼Ÿ

---

**è¡¨æ ¼å‹è¡¨ç¤ºï¼ˆtabular representationï¼‰æ˜¯å•¥ï¼Ÿ**

* â€œTabularâ€ = â€œè¡¨æ ¼å¼â€ã€‚

* è¡¨ç¤ºä½ ç›´æ¥ä¸ºæ¯ä¸ªçŠ¶æ€â€“åŠ¨ä½œå¯¹ $(s,a)$ å­˜ä¸€ä¸ªå€¼ã€‚
  å³ï¼š

  $$
  Q(s,a) \text{ è¢«å­˜æˆä¸€ä¸ªè¡¨æ ¼ } \mathbb{R}^{|S|\times|A|}
  $$

  çŠ¶æ€ç©ºé—´æœ‰é™ï¼ŒåŠ¨ä½œç©ºé—´æœ‰é™ã€‚

* â€œFunction approximationâ€ åˆ™æ˜¯ï¼š

  $$
  Q_\phi(s,a) = f_\phi(s,a)
  $$

  ç”¨å‚æ•°ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰è¡¨ç¤ºå‡½æ•°ï¼Œè€Œä¸æ˜¯ç›´æ¥å­˜è¡¨ã€‚

âœ… é€‰é¡¹ 1

> å½“ $N=1$ æ—¶ï¼Œ$Q_{\phi_{k+1}}$ æ˜¯ä¸Šä¸€æ¬¡ç­–ç•¥çš„ $Q^{\pi_k}$ çš„æ— åä¼°è®¡ã€‚

ğŸ‘‰ è¿™æ˜¯ **é”™è¯¯çš„ï¼ˆâŒï¼‰**ã€‚
å› ä¸º $N=1$ çš„æ›´æ–°ï¼ˆQ-learningï¼‰ä½¿ç”¨çš„æ˜¯

$$
y_t = r_t + \gamma \max_{a'} Q(s_{t+1},a')
$$

è¿™å¯¹åº” **Bellman æœ€ä¼˜ç®—å­ $\mathcal T^*$**ï¼Œ
ä¸æ˜¯ $\mathcal T^{\pi_k}$ï¼ˆç­–ç•¥è¯„ä¼°ç®—å­ï¼‰ã€‚

æ¢å¥è¯è¯´ï¼š

* ç›®æ ‡æ˜¯æ”¹è¿›ç­–ç•¥ï¼ˆlearn $Q^*$ï¼‰ï¼Œ
* ä¸æ˜¯è¯„ä¼°ä¸Šä¸€ä¸ªç­–ç•¥ï¼Œ
* æ‰€ä»¥å®ƒå¯¹ $Q^{\pi_k}$ æ¥è¯´æ˜¯æœ‰åçš„ã€‚

âœ… **ç»“è®ºï¼šä¸æˆç«‹ã€‚**

---

âœ… é€‰é¡¹ 2

> å½“ $N=1$ï¼Œä¸”å½“ $B\to\infty, k\to\infty$ æ—¶ï¼Œ$Q_{\phi_k}$ æ”¶æ•›åˆ° $Q^*$ã€‚

â†’ **æ­£ç¡®ï¼ˆâœ…ï¼‰**

è§£é‡Šï¼š

* $N=1$ è¡¨ç¤ºæ ‡å‡† Q-learningï¼›
* æœ‰æ— é™æ•°æ®ï¼ˆBâ†’âˆï¼‰ä¸æ— é™è¿­ä»£ï¼ˆkâ†’âˆï¼‰ï¼›
* åˆå‡è®¾å‡½æ•°é€¼è¿‘å¯ä»¥ç²¾ç¡®è¡¨è¾¾çœŸå® Qï¼›
* å› ä¸º $\mathcal T^*$ æ˜¯ Î³ æ”¶ç¼©æ˜ å°„ï¼›
* æ‰€ä»¥ç†è®ºä¸Š $Q_{\phi_k}\to Q^*$ã€‚

âœ… **ç»“è®ºï¼šæˆç«‹ã€‚**

---

âœ… é€‰é¡¹ 3

> å½“ $N>1$ï¼ˆä½†æœ‰é™ï¼‰ï¼Œä¸” $B\to\infty, k\to\infty$ æ—¶ï¼Œ$Q_{\phi_k}$ æ”¶æ•›åˆ° $Q^*$ã€‚

â†’ **é”™è¯¯ï¼ˆâŒï¼‰**

è§£é‡Šï¼š

* å¯¹äºå¤šæ­¥ $N>1$ çš„æƒ…å†µï¼Œ
  å‰ Nâˆ’1 æ­¥ç”¨çœŸå®æ ·æœ¬å¥–åŠ±ï¼Œæœ€åä¸€æ­¥è‡ªä¸¾ï¼›
* è¿™æ„å‘³ç€æ›´æ–°å¯¹åº”çš„ç®—å­ä¸å†æ˜¯ä¸¥æ ¼çš„ $\mathcal T^*$ï¼›
* è€Œæ˜¯ä¸€ä¸ªâ€œæ··åˆâ€ç®—å­ï¼Œé€šå¸¸ä¼šå¸¦æœ‰ **bootstrapping bias**ï¼›
* å³ä½¿æœ‰æ— é™æ•°æ®ï¼Œä¹Ÿä¸ä¿è¯å®Œå…¨æ— åã€‚

âœ… **ç»“è®ºï¼šä¸æˆç«‹ã€‚**

---

âœ… é€‰é¡¹ 4

> å½“ $N\to\infty$ï¼Œä¸” $B\to\infty, k\to\infty$ æ—¶ï¼Œ$Q_{\phi_k}$ æ”¶æ•›åˆ° $Q^*$ã€‚

â†’ **æ­£ç¡®ï¼ˆâœ…ï¼‰**

è§£é‡Šï¼š

* $N\to\infty$ è¡¨ç¤ºå®Œå…¨å±•å¼€å›æŠ¥ï¼ˆMonte Carloï¼‰ï¼›
* å¦‚æœæ˜¯ on-policyï¼›
* åˆ™å›æŠ¥æœŸæœ› = ç­–ç•¥çœŸå®å›æŠ¥ï¼›
* æ¯æ¬¡æ›´æ–°ååˆåšè´ªå¿ƒæ”¹è¿›ï¼ˆpolicy improvementï¼‰ï¼›
* æ‰€ä»¥å’Œ Monte Carlo Policy Iteration ç­‰ä»·ï¼›
* åœ¨æ— é™æ•°æ®ä¸å®Œç¾é€¼è¿‘ä¸‹ï¼Œæ”¶æ•›åˆ°æœ€ä¼˜ $Q^*$ã€‚

âœ… **ç»“è®ºï¼šæˆç«‹ã€‚**

\subsection{Multistep Importance Sampling (5 points)}
\label{q:importance_sampling}

We can use importance sampling to make the $N$-step update work off-policy with trajectories drawn from an arbitrary policy. Rewrite \eqref{eq:q_update} to correctly approximate a $\Q$ that improves upon $\pi$ when it is trained on data $\D$ consisting of $B$ rollouts of some other policy $\pi'(\mathbf a_t\mid\mathbf s_t)$. 

Do we need to change \eqref{eq:q_update} when $N=1$? What about as $N\to\infty$? 

You may assume that $\pi'$ always assigns positive mass to each action. [Hint: re-weight each term in the sum using a ratio of likelihoods from the policies $\pi$ and $\pi'$.]

 
\ifsolutions\solve\thesubsection\else
% TODO: answer question above
\fi

\endgroup

\textbf{Solution:}

è®¾**ç›®æ ‡ç­–ç•¥**ä¸º $\pi$ï¼ˆæˆ‘ä»¬å¸Œæœ›å­¦ä¹ /æ”¹è¿›å®ƒï¼‰ï¼Œ
**è¡Œä¸ºç­–ç•¥**ä¸º $\pi'$ï¼ˆæ•°æ®æ¥è‡ªå®ƒï¼‰ï¼Œå¹¶ä»¤

$$
\rho_{t}\;\triangleq\;\frac{\pi(a_t\mid s_t)}{\pi'(a_t\mid s_t)}\;>0\qquad(\text{é¢˜ç›®å·²ç»™æ­£æ¦‚ç‡å‡è®¾})
$$

å¯¹æ¯æ¡è½¨è¿¹çš„ä»»æ„èµ·ç‚¹ $t$ï¼Œ**N æ­¥ off-policy ç›®æ ‡ï¼ˆå¸¦é€æ­¥é‡è¦æ€§é‡‡æ ·ï¼Œperâ€“decision ISï¼‰** ä¸ºï¼š

$$
\boxed{
y^{(N)}_{t,\text{IS}}
=
\sum_{i=0}^{N-1}\Bigg(\;\gamma^i \prod_{j=0}^{i-1}\rho_{t+j}\Bigg)\, r_{t+i}
\;+\;
\gamma^N \Bigg(\prod_{j=0}^{N-1}\rho_{t+j}\Bigg)\,
\max_{a}Q(s_{t+N},a)
}
$$

ï¼ˆçº¦å®šç©ºä¹˜ç§¯ $\prod_{j=0}^{-1}(\cdot)=1$ã€‚è‹¥åœ¨ $t+N$ å‰ç»ˆæ­¢ï¼Œæœ€åä¸€é¡¹å»æ‰/è§†ä¸º 0ã€‚ï¼‰

ç„¶åæŠŠ $\,y^{(N)}_{t,\text{IS}}$ ä»£å…¥åŸæ¥çš„å›å½’å¼ï¼ˆeq. (2)ï¼‰ï¼š

$$
\phi_{k+1} \;=\; \arg\min_{\phi\in\Phi}\sum_{(j,t)}
\Big(y^{(N)}_{j,t,\text{IS}}-Q_\phi(s_{j,t},a_{j,t})\Big)^2.
$$

> ç›´è§‚ï¼šå‰ $N$ æ­¥çš„æ¯ä¸€æ®µå›æŠ¥éƒ½ç”¨å¯¹åº”é•¿åº¦çš„**æƒé‡ä¹˜ç§¯** $\prod\rho$ è¿›è¡Œæ ¡æ­£ï¼Œä½¿æ ·æœ¬åœ¨æœŸæœ›ä¸Šä» $\pi'$ çš„åˆ†å¸ƒâ€œé‡æƒâ€æˆ $\pi$ çš„åˆ†å¸ƒï¼›ç¬¬ $N$ æ­¥çš„è‡ªä¸¾é¡¹åŒæ ·ä¹˜ä¸Šé•¿åº¦ä¸º $N$ çš„ä¹˜ç§¯ã€‚

---

N=1 / Nâ†’âˆ æ˜¯å¦éœ€è¦æ”¹ï¼Ÿ

1) **N=1ï¼šä¸éœ€è¦æ”¹**ï¼ˆoff-policy å®‰å…¨ï¼‰

$$
y^{(1)}_{t}=r_t+\gamma \max_{a} Q(s_{t+1},a)
$$

è¿™ä¸€æ­¥ç›®æ ‡ä¸ $\pi'$ çš„ä¸‹ä¸€æ­¥åŠ¨ä½œæ— å…³ï¼ˆç›´æ¥å– $\max$ï¼‰ï¼Œå› æ­¤**æ— éœ€ IS æƒé‡**ï¼›è¿™å°±æ˜¯ä¸€æ­¥ Q-learning èƒ½ off-policy çš„åŸå› ã€‚

2) **$N\to\infty$**ï¼ˆçº¯ Monte Carloï¼‰ï¼š**éœ€è¦æ”¹**

æ­¤æ—¶ç›®æ ‡æ˜¯æ•´æ¡å›æŠ¥ï¼š

$$
y^{(\infty)}_{t,\text{IS}}
=\sum_{i=0}^{T-t-1}\Bigg(\gamma^i\prod_{j=0}^{i-1}\rho_{t+j}\Bigg)\,r_{t+i}
$$

ï¼ˆæ— è‡ªä¸¾é¡¹ï¼‰ã€‚
æ²¡æœ‰è¿™äº› $\prod\rho$ çš„è¯å°±æ˜¯åœ¨ $\pi'$ ä¸Šæ±‚æœŸæœ›ï¼Œ**åç¦»ç›®æ ‡ç­–ç•¥ $\pi$**ï¼›åŠ å…¥ IS åæ‰åœ¨æœŸæœ›ä¸Šç­‰äº $\mathbb{E}_\pi[G_t]$ã€‚æ³¨æ„è¿™ä¼šæ˜¾è‘—**å¢å¤§æ–¹å·®**ã€‚



